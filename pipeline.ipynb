{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>'The Classic War of the Worlds' by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0  Positive  With all this stuff going down at the moment w...\n",
       "1  Positive  'The Classic War of the Worlds' by Timothy Hin...\n",
       "2  Negative  The film starts with a manager (Nicholas Bell)...\n",
       "3  Negative  It must be assumed that those who praised this...\n",
       "4  Positive  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"MovieReview.csv\")\n",
    "display(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "df = df.drop('sentiment', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/hui/anaconda3/envs/tf/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/hui/anaconda3/envs/tf/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl (287 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     mots \u001b[38;5;241m=\u001b[39m [mot \u001b[38;5;28;01mfor\u001b[39;00m mot \u001b[38;5;129;01min\u001b[39;00m mots \u001b[38;5;28;01mif\u001b[39;00m mot \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(mots)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 30\u001b[0m df\u001b[38;5;241m.\u001b[39mreview \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreview\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mpreprocess_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m     mots \u001b[38;5;241m=\u001b[39m [mot \u001b[38;5;28;01mfor\u001b[39;00m mot \u001b[38;5;129;01min\u001b[39;00m mots \u001b[38;5;28;01mif\u001b[39;00m mot \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(mots)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 30\u001b[0m df\u001b[38;5;241m.\u001b[39mreview \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreview\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x :\u001b[43mpreprocess_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mpreprocess_sentence\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     23\u001b[0m w \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0,2}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, w)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# remove stopword\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m mots \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m mots \u001b[38;5;241m=\u001b[39m [mot \u001b[38;5;28;01mfor\u001b[39;00m mot \u001b[38;5;129;01min\u001b[39;00m mots \u001b[38;5;28;01mif\u001b[39;00m mot \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(mots)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/nltk/tokenize/__init__.py:143\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/nltk/tokenize/__init__.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/nltk/tokenize/destructive.py:179\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    176\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 179\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    182\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/re/__init__.py:315\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    312\u001b[0m     template \u001b[38;5;241m=\u001b[39m _parser\u001b[38;5;241m.\u001b[39mparse_template(template, pattern)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parser\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     template \u001b[38;5;241m=\u001b[39m _compile_repl(template, pattern)\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "\n",
    "    # remove stopword\n",
    "    mots = word_tokenize(w.strip())\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return ' '.join(mots).strip()\n",
    "\n",
    "df.review = df.review.apply(lambda x :preprocess_sentence(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocab_infor.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([word2idx, idx2word, vocab_size], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "# write variables to filename [a,b,c can be of any size]\n",
    "filename = 'vocab_info'\n",
    "hkl.dump([word2idx, idx2word, vocab_size], filename)\n",
    "\n",
    "# load variables from filename\n",
    "word2idx, idx2word, vocab_size = hkl.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sentenceToData(tokens, WINDOW_SIZE):\n",
    "    window = np.concatenate((np.arange(-WINDOW_SIZE,0),np.arange(1,WINDOW_SIZE+1)))\n",
    "    X,Y=([],[])\n",
    "    for word_index, word in enumerate(tokens) :\n",
    "        if ((word_index - WINDOW_SIZE >= 0) and (word_index + WINDOW_SIZE <= len(tokens) - 1)) :\n",
    "            X.append(word)\n",
    "            Y.append([tokens[word_index-i] for i in window])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "X, Y = ([], [])\n",
    "for review in df.review:\n",
    "    for sentence in review.split(\".\"):\n",
    "        word_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        if len(word_list) >= WINDOW_SIZE:\n",
    "            Y1, X1 = sentenceToData(word_list, WINDOW_SIZE//2)\n",
    "            X.extend(X1)\n",
    "            Y.extend(Y1)\n",
    "    \n",
    "X = np.array(X).astype(int)\n",
    "y = np.array(Y).astype(int).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size = 128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "import pickle\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10000)             3010000   \n",
      "=================================================================\n",
      "Total params: 6,010,000\n",
      "Trainable params: 6,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Variable 'embedding_4/embeddings:0' shape=(10000, 300) dtype=float32, numpy=\n",
      "array([[-0.04106069, -0.00338229, -0.00746735, ...,  0.00486269,\n",
      "        -0.02949026,  0.00457094],\n",
      "       [-0.00233353, -0.01724849, -0.02378774, ..., -0.04918478,\n",
      "         0.04500575, -0.0155657 ],\n",
      "       [ 0.01697308, -0.0481892 , -0.00905486, ...,  0.02309169,\n",
      "        -0.02946174, -0.00040041],\n",
      "       ...,\n",
      "       [ 0.02221212,  0.00347158,  0.03367311, ...,  0.01386524,\n",
      "         0.01152879,  0.04179642],\n",
      "       [ 0.01719414, -0.01939278,  0.01850558, ..., -0.04879949,\n",
      "        -0.01169062, -0.04303279],\n",
      "       [ 0.04208897, -0.01257206,  0.01167732, ..., -0.00296347,\n",
      "         0.0131172 ,  0.04405511]], dtype=float32)>, <tf.Variable 'dense_4/kernel:0' shape=(300, 10000) dtype=float32, numpy=\n",
      "array([[ 1.22071747e-02,  8.62878002e-03,  1.06350388e-02, ...,\n",
      "         1.38450507e-02,  1.38582345e-02, -1.88630261e-02],\n",
      "       [ 1.12416036e-03,  5.62338531e-03,  1.36317518e-02, ...,\n",
      "        -1.54856928e-02,  7.79256225e-05,  9.02358629e-03],\n",
      "       [ 1.55845340e-02, -7.50465319e-03, -3.55821103e-04, ...,\n",
      "         1.28781106e-02,  5.96530363e-03, -1.66307129e-02],\n",
      "       ...,\n",
      "       [ 8.56794603e-03,  1.47180911e-02, -4.60835174e-03, ...,\n",
      "         1.05881635e-02,  1.20908394e-03,  6.72589801e-03],\n",
      "       [-2.04323139e-02, -8.97112396e-03,  2.08826344e-02, ...,\n",
      "        -9.46606230e-03, -1.40591813e-02,  2.25778501e-02],\n",
      "       [ 2.15622056e-02, -2.23648474e-02, -2.17567664e-02, ...,\n",
      "        -1.00335283e-02,  1.18001644e-02,  1.74788199e-03]], dtype=float32)>, <tf.Variable 'dense_4/bias:0' shape=(10000,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword2vec.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py:2349\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2346\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2347\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`load_weights` requires h5py when loading weights from HDF5.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 2349\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2350\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to load weights saved in HDF5 format into a subclassed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2351\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel which has not created its variables yet. Call the Model \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2352\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst, then load the weights.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_weights_created()\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.load_weights(\"word2vec.h5\")\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('vocab_infor.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    word2idx, idx2word, vocab_size = pickle.load(f)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown optimizer: Custom>Adam. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword2vec.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab_infor.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Python 3: open(..., 'rb')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     word2idx, idx2word, vocab_size \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/saving/save.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m load_context\u001b[38;5;241m.\u001b[39mload_context(options):\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    199\u001b[0m       (\u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile) \u001b[38;5;129;01mor\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mis_hdf5(filepath))):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m   filepath \u001b[38;5;241m=\u001b[39m path_to_string(filepath)\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/saving/hdf5_format.py:198\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    195\u001b[0m training_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(training_config)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m, from_serialized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m saving_utils\u001b[38;5;241m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/saving/saving_utils.py:202\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[0;34m(training_config, custom_objects)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n\u001b[1;32m    201\u001b[0m   optimizer_config \u001b[38;5;241m=\u001b[39m training_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_config\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 202\u001b[0m   optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;66;03m# Recover losses.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers.py:95\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m all_classes:\n\u001b[1;32m     94\u001b[0m   config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/generic_utils.py:659\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    657\u001b[0m   \u001b[38;5;66;03m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[1;32m    658\u001b[0m   config \u001b[38;5;241m=\u001b[39m identifier\n\u001b[0;32m--> 659\u001b[0m   (\u001b[38;5;28mcls\u001b[39m, cls_config) \u001b[38;5;241m=\u001b[39m \u001b[43mclass_and_config_for_serialized_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m   \u001b[38;5;66;03m# If this object has already been loaded (i.e. it's shared between multiple\u001b[39;00m\n\u001b[1;32m    663\u001b[0m   \u001b[38;5;66;03m# objects), return the already-loaded object.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m   shared_object_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(SHARED_OBJECT_KEY)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/generic_utils.py:556\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m get_registered_object(class_name, custom_objects, module_objects)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Please ensure this object is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    558\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassed to the `custom_objects` argument. See \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    559\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    560\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    561\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(printable_module_name, class_name))\n\u001b[1;32m    563\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown optimizer: Custom>Adam. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# import pickle\n",
    "\n",
    "# model = load_model('word2vec.h5')\n",
    "\n",
    "# with open('vocab_infor.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     word2idx, idx2word, vocab_size = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romance  --  0.25692179799079895\n",
      "lust  --  0.243804469704628\n",
      "relationship  --  0.21269157528877258\n"
     ]
    }
   ],
   "source": [
    "# Similarity is a metric which measures the distance between two words. This distance represents the way \n",
    "# words are related to each other\n",
    "vectors = model.layers[0].trainable_weights[0].numpy()\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def dot_product(vec1, vec2):\n",
    "    return np.sum((vec1*vec2))\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return dot_product(vec1, vec2)/np.sqrt(dot_product(vec1, vec1)*dot_product(vec2, vec2))\n",
    "\n",
    "def find_closest(word_index, vectors, number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def compare(index_word1, index_word2, index_word3, vectors, number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[index_word1] - vectors[index_word2] + vectors[index_word3]\n",
    "    normalizer = Normalizer()\n",
    "    query_vector =  normalizer.fit_transform([query_vector], 'l2')\n",
    "    query_vector= query_vector[0]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def print_closest(word, number=10):\n",
    "    index_closest_words = find_closest(word2idx[word], vectors, number)\n",
    "    for index_word in index_closest_words :\n",
    "        print(idx2word[index_word[1]],\" -- \",index_word[0])\n",
    "\n",
    "print_closest('love',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hui/Documents/projects/courses/datascientest/word2vec_steamlit/Untitled'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "import pickle\n",
    "\n",
    "with open('vocab_infor.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    word2idx, idx2word, vocab_size = pickle.load(f)\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.load_weights(\"word2vec.h5\")\n",
    "\n",
    "\n",
    "\n",
    "with open('model_pickle', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 21:50:10.842093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('model_pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.layers[0].trainable_weights[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "absl-py                 2.1.0\n",
      "aiohappyeyeballs        2.4.3\n",
      "aiohttp                 3.10.5\n",
      "aiosignal               1.2.0\n",
      "appnope                 0.1.4\n",
      "asttokens               3.0.0\n",
      "astunparse              1.6.3\n",
      "attrs                   24.2.0\n",
      "blinker                 1.6.2\n",
      "Brotli                  1.0.9\n",
      "cachetools              5.3.3\n",
      "certifi                 2024.8.30\n",
      "cffi                    1.17.1\n",
      "charset-normalizer      3.3.2\n",
      "click                   8.1.7\n",
      "comm                    0.2.2\n",
      "contourpy               1.3.1\n",
      "cryptography            41.0.3\n",
      "cycler                  0.12.1\n",
      "debugpy                 1.8.10\n",
      "decorator               5.1.1\n",
      "exceptiongroup          1.2.2\n",
      "executing               2.1.0\n",
      "flatbuffers             24.3.25\n",
      "fonttools               4.55.3\n",
      "frozenlist              1.5.0\n",
      "gast                    0.4.0\n",
      "google-auth             2.29.0\n",
      "google-auth-oauthlib    0.5.2\n",
      "google-pasta            0.2.0\n",
      "grpcio                  1.48.2\n",
      "h5py                    3.12.1\n",
      "idna                    3.7\n",
      "imbalanced-learn        0.12.4\n",
      "imblearn                0.0\n",
      "importlib_metadata      8.5.0\n",
      "ipykernel               6.29.5\n",
      "ipython                 8.30.0\n",
      "jedi                    0.19.2\n",
      "joblib                  1.4.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.7.2\n",
      "keras                   2.12.0\n",
      "Keras-Preprocessing     1.1.2\n",
      "kiwisolver              1.4.7\n",
      "Markdown                3.4.1\n",
      "MarkupSafe              2.1.3\n",
      "matplotlib              3.9.3\n",
      "matplotlib-inline       0.1.7\n",
      "multidict               6.1.0\n",
      "nest_asyncio            1.6.0\n",
      "nltk                    3.9.1\n",
      "numpy                   1.23.5\n",
      "oauthlib                3.2.2\n",
      "opt-einsum              3.3.0\n",
      "packaging               24.2\n",
      "pandas                  1.5.2\n",
      "parso                   0.8.4\n",
      "pexpect                 4.9.0\n",
      "pickleshare             0.7.5\n",
      "pillow                  11.0.0\n",
      "pip                     24.3.1\n",
      "platformdirs            4.3.6\n",
      "prompt_toolkit          3.0.48\n",
      "propcache               0.2.0\n",
      "protobuf                3.20.3\n",
      "psutil                  6.1.0\n",
      "ptyprocess              0.7.0\n",
      "pure_eval               0.2.3\n",
      "pyasn1                  0.4.8\n",
      "pyasn1-modules          0.2.8\n",
      "pycparser               2.21\n",
      "Pygments                2.18.0\n",
      "PyJWT                   2.9.0\n",
      "pyOpenSSL               23.2.0\n",
      "pyparsing               3.2.0\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2024.2\n",
      "pyzmq                   26.2.0\n",
      "regex                   2024.11.6\n",
      "requests                2.32.3\n",
      "requests-oauthlib       2.0.0\n",
      "rsa                     4.7.2\n",
      "scikit-learn            1.6.0\n",
      "scipy                   1.14.1\n",
      "setuptools              75.6.0\n",
      "six                     1.17.0\n",
      "stack_data              0.6.3\n",
      "tensorboard             2.12.1\n",
      "tensorboard_data_server 0.7.0\n",
      "tensorboard-plugin-wit  1.6.0\n",
      "tensorflow              2.12.0\n",
      "tensorflow-estimator    2.12.0\n",
      "termcolor               2.1.0\n",
      "threadpoolctl           3.5.0\n",
      "tornado                 6.4.2\n",
      "tqdm                    4.67.1\n",
      "traitlets               5.14.3\n",
      "typing_extensions       4.12.2\n",
      "urllib3                 2.2.3\n",
      "wcwidth                 0.2.13\n",
      "Werkzeug                3.0.6\n",
      "wheel                   0.35.1\n",
      "wrapt                   1.14.1\n",
      "yarl                    1.18.0\n",
      "zipp                    3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word2vec_joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"word2vec_joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec.h5\",include_optimizer=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
