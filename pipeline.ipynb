{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>'The Classic War of the Worlds' by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0  Positive  With all this stuff going down at the moment w...\n",
       "1  Positive  'The Classic War of the Worlds' by Timothy Hin...\n",
       "2  Negative  The film starts with a manager (Nicholas Bell)...\n",
       "3  Negative  It must be assumed that those who praised this...\n",
       "4  Positive  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"MovieReview.csv\")\n",
    "display(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "df = df.drop('sentiment', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl (287 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff going moment started listening music wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  stuff going moment started listening music wat...\n",
       "1  classic war worlds timothy hines entertaining ...\n",
       "2  film starts manager nicholas bell giving welco...\n",
       "3  must assumed praised film greatest filmed oper...\n",
       "4  superbly trashy wondrously unpretentious explo..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "\n",
    "    # remove stopword\n",
    "    mots = word_tokenize(w.strip())\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return ' '.join(mots).strip()\n",
    "\n",
    "df.review = df.review.apply(lambda x :preprocess_sentence(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocab_infor.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([word2idx, idx2word, vocab_size], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "# write variables to filename [a,b,c can be of any size]\n",
    "filename = 'vocab_info'\n",
    "hkl.dump([word2idx, idx2word, vocab_size], filename)\n",
    "\n",
    "# load variables from filename\n",
    "word2idx, idx2word, vocab_size = hkl.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sentenceToData(tokens, WINDOW_SIZE):\n",
    "    window = np.concatenate((np.arange(-WINDOW_SIZE,0),np.arange(1,WINDOW_SIZE+1)))\n",
    "    X,Y=([],[])\n",
    "    for word_index, word in enumerate(tokens) :\n",
    "        if ((word_index - WINDOW_SIZE >= 0) and (word_index + WINDOW_SIZE <= len(tokens) - 1)) :\n",
    "            X.append(word)\n",
    "            Y.append([tokens[word_index-i] for i in window])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "X, Y = ([], [])\n",
    "for review in df.review:\n",
    "    for sentence in review.split(\".\"):\n",
    "        word_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        if len(word_list) >= WINDOW_SIZE:\n",
    "            Y1, X1 = sentenceToData(word_list, WINDOW_SIZE//2)\n",
    "            X.extend(X1)\n",
    "            Y.extend(Y1)\n",
    "    \n",
    "X = np.array(X).astype(int)\n",
    "y = np.array(Y).astype(int).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12163/12163 [==============================] - 515s 42ms/step - loss: 7.6543 - accuracy: 0.0313\n",
      "Epoch 2/50\n",
      "12163/12163 [==============================] - 469s 39ms/step - loss: 6.9537 - accuracy: 0.0581\n",
      "Epoch 3/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 6.5205 - accuracy: 0.0754\n",
      "Epoch 4/50\n",
      "12163/12163 [==============================] - 438s 36ms/step - loss: 6.1902 - accuracy: 0.0886\n",
      "Epoch 5/50\n",
      "12163/12163 [==============================] - 521s 43ms/step - loss: 5.9204 - accuracy: 0.1005\n",
      "Epoch 6/50\n",
      "12163/12163 [==============================] - 527s 43ms/step - loss: 5.6976 - accuracy: 0.1111\n",
      "Epoch 7/50\n",
      "12163/12163 [==============================] - 447s 37ms/step - loss: 5.5147 - accuracy: 0.1208\n",
      "Epoch 8/50\n",
      "12163/12163 [==============================] - 413s 34ms/step - loss: 5.3656 - accuracy: 0.1303\n",
      "Epoch 9/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 5.2435 - accuracy: 0.1388\n",
      "Epoch 10/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 5.1422 - accuracy: 0.1468\n",
      "Epoch 11/50\n",
      "12163/12163 [==============================] - 415s 34ms/step - loss: 5.0581 - accuracy: 0.1538\n",
      "Epoch 12/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.9871 - accuracy: 0.1602\n",
      "Epoch 13/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.9262 - accuracy: 0.1661\n",
      "Epoch 14/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.8732 - accuracy: 0.1712\n",
      "Epoch 15/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.8271 - accuracy: 0.1760\n",
      "Epoch 16/50\n",
      "12163/12163 [==============================] - 459s 38ms/step - loss: 4.7870 - accuracy: 0.1802\n",
      "Epoch 17/50\n",
      "12163/12163 [==============================] - 458s 38ms/step - loss: 4.7507 - accuracy: 0.1841\n",
      "Epoch 18/50\n",
      "12163/12163 [==============================] - 413s 34ms/step - loss: 4.7184 - accuracy: 0.1875\n",
      "Epoch 19/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6892 - accuracy: 0.1908\n",
      "Epoch 20/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6630 - accuracy: 0.1938\n",
      "Epoch 21/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6392 - accuracy: 0.1962\n",
      "Epoch 22/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6170 - accuracy: 0.1990\n",
      "Epoch 23/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.5969 - accuracy: 0.2010\n",
      "Epoch 24/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.5784 - accuracy: 0.2028\n",
      "Epoch 25/50\n",
      "12163/12163 [==============================] - 449s 37ms/step - loss: 4.5613 - accuracy: 0.2051\n",
      "Epoch 26/50\n",
      "12163/12163 [==============================] - 492s 40ms/step - loss: 4.5456 - accuracy: 0.2065\n",
      "Epoch 27/50\n",
      "12163/12163 [==============================] - 557s 46ms/step - loss: 4.5307 - accuracy: 0.2082\n",
      "Epoch 28/50\n",
      "12163/12163 [==============================] - 497s 41ms/step - loss: 4.5172 - accuracy: 0.2098\n",
      "Epoch 29/50\n",
      " 8895/12163 [====================>.........] - ETA: 2:38 - loss: 4.4338 - accuracy: 0.2215"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size = 128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "import pickle\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.load_weights(\"word2vec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 300) dtype=float32, numpy=\n",
       " array([[-1.6336739e-02, -3.2969713e-03,  2.2689249e-02, ...,\n",
       "          3.8612876e-02,  8.3623528e-03, -4.6237852e-02],\n",
       "        [ 4.7363630e-01,  8.3849803e-03,  2.8525680e-02, ...,\n",
       "         -1.0739079e-01,  1.0848190e-01, -2.3038709e-01],\n",
       "        [ 1.4934343e-01, -1.8013370e-01, -4.6365210e-01, ...,\n",
       "         -2.0404343e-01,  1.5457350e-01, -2.9626891e-01],\n",
       "        ...,\n",
       "        [-1.6274743e+00,  5.7811457e-01,  3.5598168e-01, ...,\n",
       "          2.0314949e+00,  7.2257441e-01, -4.6763816e-01],\n",
       "        [-2.0888686e-01,  1.3393493e+00,  1.2781122e+00, ...,\n",
       "         -3.4095354e+00, -9.4755644e-01,  1.2645215e+00],\n",
       "        [-1.3592587e+00, -2.6635106e+00,  3.4834404e+00, ...,\n",
       "         -4.9866799e-01, -5.7168049e-01,  1.6844219e+00]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(300, 10000) dtype=float32, numpy=\n",
       " array([[-0.76584256, -0.10200101, -0.19614986, ...,  0.5564938 ,\n",
       "         -1.3159206 ,  0.5389798 ],\n",
       "        [ 1.4475437 ,  0.7525983 ,  0.6208235 , ...,  3.577276  ,\n",
       "          0.9459813 ,  1.525698  ],\n",
       "        [-0.909907  , -0.44789982, -0.47953415, ..., -1.079589  ,\n",
       "         -0.8019548 , -1.7242334 ],\n",
       "        ...,\n",
       "        [-0.52419335, -0.5570592 , -0.37788606, ..., -1.3472174 ,\n",
       "         -0.91497964, -0.8806785 ],\n",
       "        [-0.9246708 , -0.4139992 , -0.4651735 , ..., -0.7970429 ,\n",
       "         -0.46967337, -1.2772781 ],\n",
       "        [ 1.6075364 ,  0.92075527,  0.8307833 , ...,  2.2276018 ,\n",
       "          1.7819991 ,  2.1110814 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10000,) dtype=float32, numpy=\n",
       " array([-10.47077  ,   3.5529273,   3.7487316, ...,  -4.4392266,\n",
       "         -3.6232936,  -3.1888316], dtype=float32)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 300)         3000000   \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 300)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10000)             3010000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,010,000\n",
      "Trainable params: 6,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('vocab_infor.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    word2idx, idx2word, vocab_size = pickle.load(f)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.12.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, jax, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "model = load_model('word2vec.h5')\n",
    "\n",
    "with open('vocab_infor.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    word2idx, idx2word, vocab_size = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romance  --  0.25692179799079895\n",
      "lust  --  0.243804469704628\n",
      "relationship  --  0.21269157528877258\n"
     ]
    }
   ],
   "source": [
    "# Similarity is a metric which measures the distance between two words. This distance represents the way \n",
    "# words are related to each other\n",
    "vectors = model.layers[0].trainable_weights[0].numpy()\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def dot_product(vec1, vec2):\n",
    "    return np.sum((vec1*vec2))\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return dot_product(vec1, vec2)/np.sqrt(dot_product(vec1, vec1)*dot_product(vec2, vec2))\n",
    "\n",
    "def find_closest(word_index, vectors, number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def compare(index_word1, index_word2, index_word3, vectors, number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[index_word1] - vectors[index_word2] + vectors[index_word3]\n",
    "    normalizer = Normalizer()\n",
    "    query_vector =  normalizer.fit_transform([query_vector], 'l2')\n",
    "    query_vector= query_vector[0]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def print_closest(word, number=10):\n",
    "    index_closest_words = find_closest(word2idx[word], vectors, number)\n",
    "    for index_word in index_closest_words :\n",
    "        print(idx2word[index_word[1]],\" -- \",index_word[0])\n",
    "\n",
    "print_closest('love',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (2.12.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.2-cp311-cp311-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (1.48.2)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Using cached optree-0.13.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.16.2-cp311-cp311-macosx_10_15_x86_64.whl (259.6 MB)\n",
      "Using cached keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_10_14_x86_64.whl (2.5 MB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.13.1-cp311-cp311-macosx_10_9_universal2.whl (589 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, tensorflow-io-gcs-filesystem, optree, ml-dtypes, mdurl, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.1\n",
      "    Uninstalling tensorboard-2.12.1:\n",
      "      Successfully uninstalled tensorboard-2.12.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "Successfully installed keras-3.7.0 libclang-18.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 optree-0.13.1 rich-13.9.4 tensorboard-2.16.2 tensorflow-2.16.2 tensorflow-io-gcs-filesystem-0.37.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
