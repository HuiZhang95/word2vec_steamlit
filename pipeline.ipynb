{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>'The Classic War of the Worlds' by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0  Positive  With all this stuff going down at the moment w...\n",
       "1  Positive  'The Classic War of the Worlds' by Timothy Hin...\n",
       "2  Negative  The film starts with a manager (Nicholas Bell)...\n",
       "3  Negative  It must be assumed that those who praised this...\n",
       "4  Positive  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"MovieReview.csv\")\n",
    "display(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "df = df.drop('sentiment', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /Users/hui/anaconda3/envs/tensorflow/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl (287 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "\n",
    "    # remove stopword\n",
    "    mots = word_tokenize(w.strip())\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return ' '.join(mots).strip()\n",
    "\n",
    "df.review = df.review.apply(lambda x :preprocess_sentence(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 11:06:39.743997: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocab_infor.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([word2idx, idx2word, vocab_size], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "# write variables to filename [a,b,c can be of any size]\n",
    "filename = 'vocab_info'\n",
    "hkl.dump([word2idx, idx2word, vocab_size], filename)\n",
    "\n",
    "# load variables from filename\n",
    "word2idx, idx2word, vocab_size = hkl.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sentenceToData(tokens, WINDOW_SIZE):\n",
    "    window = np.concatenate((np.arange(-WINDOW_SIZE,0),np.arange(1,WINDOW_SIZE+1)))\n",
    "    X,Y=([],[])\n",
    "    for word_index, word in enumerate(tokens) :\n",
    "        if ((word_index - WINDOW_SIZE >= 0) and (word_index + WINDOW_SIZE <= len(tokens) - 1)) :\n",
    "            X.append(word)\n",
    "            Y.append([tokens[word_index-i] for i in window])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "X, Y = ([], [])\n",
    "for review in df.review:\n",
    "    for sentence in review.split(\".\"):\n",
    "        word_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        if len(word_list) >= WINDOW_SIZE:\n",
    "            Y1, X1 = sentenceToData(word_list, WINDOW_SIZE//2)\n",
    "            X.extend(X1)\n",
    "            Y.extend(Y1)\n",
    "    \n",
    "X = np.array(X).astype(int)\n",
    "y = np.array(Y).astype(int).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12163/12163 [==============================] - 515s 42ms/step - loss: 7.6543 - accuracy: 0.0313\n",
      "Epoch 2/50\n",
      "12163/12163 [==============================] - 469s 39ms/step - loss: 6.9537 - accuracy: 0.0581\n",
      "Epoch 3/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 6.5205 - accuracy: 0.0754\n",
      "Epoch 4/50\n",
      "12163/12163 [==============================] - 438s 36ms/step - loss: 6.1902 - accuracy: 0.0886\n",
      "Epoch 5/50\n",
      "12163/12163 [==============================] - 521s 43ms/step - loss: 5.9204 - accuracy: 0.1005\n",
      "Epoch 6/50\n",
      "12163/12163 [==============================] - 527s 43ms/step - loss: 5.6976 - accuracy: 0.1111\n",
      "Epoch 7/50\n",
      "12163/12163 [==============================] - 447s 37ms/step - loss: 5.5147 - accuracy: 0.1208\n",
      "Epoch 8/50\n",
      "12163/12163 [==============================] - 413s 34ms/step - loss: 5.3656 - accuracy: 0.1303\n",
      "Epoch 9/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 5.2435 - accuracy: 0.1388\n",
      "Epoch 10/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 5.1422 - accuracy: 0.1468\n",
      "Epoch 11/50\n",
      "12163/12163 [==============================] - 415s 34ms/step - loss: 5.0581 - accuracy: 0.1538\n",
      "Epoch 12/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.9871 - accuracy: 0.1602\n",
      "Epoch 13/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.9262 - accuracy: 0.1661\n",
      "Epoch 14/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.8732 - accuracy: 0.1712\n",
      "Epoch 15/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.8271 - accuracy: 0.1760\n",
      "Epoch 16/50\n",
      "12163/12163 [==============================] - 459s 38ms/step - loss: 4.7870 - accuracy: 0.1802\n",
      "Epoch 17/50\n",
      "12163/12163 [==============================] - 458s 38ms/step - loss: 4.7507 - accuracy: 0.1841\n",
      "Epoch 18/50\n",
      "12163/12163 [==============================] - 413s 34ms/step - loss: 4.7184 - accuracy: 0.1875\n",
      "Epoch 19/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6892 - accuracy: 0.1908\n",
      "Epoch 20/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6630 - accuracy: 0.1938\n",
      "Epoch 21/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6392 - accuracy: 0.1962\n",
      "Epoch 22/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.6170 - accuracy: 0.1990\n",
      "Epoch 23/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.5969 - accuracy: 0.2010\n",
      "Epoch 24/50\n",
      "12163/12163 [==============================] - 412s 34ms/step - loss: 4.5784 - accuracy: 0.2028\n",
      "Epoch 25/50\n",
      "12163/12163 [==============================] - 449s 37ms/step - loss: 4.5613 - accuracy: 0.2051\n",
      "Epoch 26/50\n",
      "12163/12163 [==============================] - 492s 40ms/step - loss: 4.5456 - accuracy: 0.2065\n",
      "Epoch 27/50\n",
      "12163/12163 [==============================] - 557s 46ms/step - loss: 4.5307 - accuracy: 0.2082\n",
      "Epoch 28/50\n",
      "12163/12163 [==============================] - 497s 41ms/step - loss: 4.5172 - accuracy: 0.2098\n",
      "Epoch 29/50\n",
      " 8895/12163 [====================>.........] - ETA: 2:38 - loss: 4.4338 - accuracy: 0.2215"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size = 128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.h5\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
